# spring-ai-rag

A simple Spring Boot application that demonstrates **Retrieval-Augmented Generation (RAG)** using **Spring AI**, **Google Gemini (Google GenAI)**, and a **PostgreSQL + pgvector**-backed vector store.

## What this project does

- Exposes a REST endpoint that sends user prompts to an AI chat model (Google GenAI via Spring AI).
- Uses a vector store (pgvector on PostgreSQL) to retrieve relevant context for better answers.
- Is configured as a standard Spring Boot app with familiar `controller`, `service`, and `resources` structure.

## Key technologies

- **Java / Spring Boot**
- **Spring AI** (`ChatClient`, `GoogleGenAiChatModel`, `VectorStore`)
- **PostgreSQL + pgvector** as the vector database
- **Maven** as the build tool

## Important files

- `src/main/java/com/ai/spring_ai_rag/SpringAiRagApplication.java` – main Spring Boot application class.
- `src/main/java/com/ai/spring_ai_rag/controller/ControllerClass.java` – REST controller exposing the `/v1/ai/prompt-chat` endpoint.
- `src/main/java/com/ai/spring_ai_rag/service/DocumentInjectionService.java` – service hook for initialization / document loading (can be extended).
- `src/main/resources/application.properties` – application name, server port, DB + pgvector config, and Spring AI settings.

## How to run

1. Make sure you have **Java 17+** and **Maven** installed.
2. Configure your PostgreSQL connection and credentials in `src/main/resources/application.properties`:
   - `spring.datasource.url`
   - `spring.datasource.username`
   - `spring.datasource.password`
3. (Optional) Configure your AI provider settings (e.g., Google GenAI or Azure OpenAI) in `application.properties`.
4. From the project root, build and run the app:

```powershell
mvnw.cmd clean install
mvnw.cmd spring-boot:run
```

The application will start on `http://localhost:8080` by default.

## Calling the AI endpoint

Send a POST request with a plain text body to the chat endpoint:

```http
POST http://localhost:8080/v1/ai/prompt-chat
Content-Type: text/plain

Your question or prompt text here
```

The response will be a string generated by the AI model, optionally enriched with context from the vector store.

## Next steps

- Implement document ingestion logic in `DocumentInjectionService` to populate the vector store.
- Add more endpoints for administration or multi-turn conversations.
